{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Implementation and Results </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data Preprocessing \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h4>Random Forests </h4>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The model <i>sklearn.ensemble.RandomForestRegressor </i> from scikit learn was used to fit our data.The model is specifically designed for fitting regression data. Using the <i> RandomForestRegressor</i>, a number of classifying decision trees are fit on different batches of the training data. Means are used to improve the predictive accuracy and control overfitting. Out of the parameters of the model, these were focused on in our analysis: n_estimators,max_depth,max_iterations,OOB_samples and max_\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The parameter <i> n_estimators</i> is the number of trees that to be included in our tree. The first analysis that was carried out for Random Forests involved training the data on different number of trees to find the optimal <i> n_estimators</i>. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Number of Features</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfTrees = [1,5,10, 50, 100,150,200,250,300]\n",
    "for num in numberOfTrees:\n",
    "    rf = RandomForestRegressor(n_estimators=num, random_state=0,)\n",
    "    rf.fit(X_train, y_train)\n",
    "    sco.append(rf.score)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    score = cross_val_score(rf,X_train,y_train,cv=5,scoring='r2')\n",
    "    r2.append(np.mean(score))\n",
    "    mseScore=cross_val_score(rf,X_train,y_train,cv=5,\n",
    "                             scoring='neg_mean_squared_error')\n",
    "    mse.append(np.mean(mseScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Feature Importance</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor( n_estimators=100,max_features=19,\n",
    "                    random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], \n",
    "                                   feature_importances[indices[f]]))\n",
    "\n",
    "y_ticks = np.arange(0, len(feature_importances))\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(y_ticks, feature_importances[indices])\n",
    "ax.set_yticklabels(indices)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"Random Forest Feature Importances \")\n",
    "fig.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max features list has alll options available for mac_features parameter\n",
    "max_featureL=['auto','sqrt','log2',19]\n",
    "max_feat_names=['auto=n_features','sqrt[n_features]',\n",
    "                'log2[n_features]','n_features']\n",
    "max_depthL = [None,1,2,3,4,5,6,7,8,9,10]\n",
    "i = len(max_depthL)\n",
    "testerror = np.empty((4, i))\n",
    "trainerror = np.empty((4, i))\n",
    "mse = np.empty((4, i))\n",
    "r2 = np.empty((4, i))\n",
    "for f,feature in enumerate(max_featureL):\n",
    "    for s,dep in enumerate(max_depthL):\n",
    "        rf = RandomForestRegressor( n_estimators=100,\n",
    "                    max_features=feature,\n",
    "                    max_depth=dep,random_state=0)\n",
    "        rf.fit(X_train, y_train)\n",
    "        score = cross_val_score(rf,X_train,y_train,\n",
    "                                cv=5,scoring='r2')\n",
    "        r2[f,s] = np.mean(score)\n",
    "        mseScore=cross_val_score(rf,X_train,y_train,\n",
    "                        cv=5,scoring='neg_mean_squared_error')\n",
    "        mse[f,s] = np.mean(mseScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyDegree = [1,2,3,4,5]\n",
    "for deg in polyDegree:\n",
    "    poly = PolynomialFeatures(degree = deg)\n",
    "    X_train = poly.fit_transform(X_train)\n",
    "    X_test = poly.fit_transform(X_test)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_tilde= model.predict(X_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse.append(mean_squared_error(y_test,y_pred))\n",
    "    r2.append(r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alph= [0,1e-20,1e-15,1e-10,1e-5,1e-4,1e-3,1e-2,1e-1,1,10,100,1000,10000]\n",
    "alphaLength = len(alph)\n",
    "degreeLength=len(degree)\n",
    "mse=np.empty((degreeLength, alphaLength))\n",
    "r2 = np.empty((degreeLength, alphaLength))\n",
    "test_error = np.empty((degreeLength, alphaLength))\n",
    "train_error = np.empty((degreeLength, alphaLength))\n",
    "\n",
    "for d, deg in enumerate(degree):\n",
    "    poly = PolynomialFeatures(degree = deg)\n",
    "    X_train = poly.fit_transform(X_train)\n",
    "    X_test = poly.fit_transform(X_test)\n",
    "        # for each degree, fit model into train set with correct number of predictors\n",
    "\n",
    "    for a, afa in enumerate(alph):\n",
    "        model = Ridge(alpha = afa,random_state = None)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_tilde= model.predict(X_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        train_error[d,a]=mean_squared_error(y_train,y_tilde)\n",
    "        test_error[d,a]=mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
